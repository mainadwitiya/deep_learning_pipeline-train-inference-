{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "file_path='/home/daisy/mydrive/web-dev/deep_learning_pipeline-train-inference-/training_controller/apis/configs/tf2/centernet_hourglass104_1024x1024_coco17_tpu-32.config'\n",
    "path_2='/home/daisy/mydrive/web-dev/deep_learning_pipeline-train-inference-/kk.config'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "train_input_config",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-012835494566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mproto_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtext_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mpipeline_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_input_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_map_path\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"123/label_map.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mpipeline_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_resizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_shape_resizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpipeline_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_resizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_shape_resizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: train_input_config"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from google.protobuf import text_format\n",
    "from object_detection.protos import pipeline_pb2\n",
    "\n",
    "\n",
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "print(pipeline_config)\n",
    "with tf.io.gfile.GFile(file_path, \"r\") as f:                                                                                                                                                                                                                     \n",
    "    proto_str = f.read()                                                                                                                                                                                                                                          \n",
    "    text_format.Merge(proto_str, pipeline_config) \n",
    "pipeline_config.train_input_config.label_map_path= \"123/label_map.txt\"\n",
    "pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = 300                                                                                                                                                                                          \n",
    "pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = 300\n",
    "\n",
    "config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n",
    "with tf.io.gfile.GFile(path_2, \"wb\") as f:                                                                                                                                                                                                                       \n",
    "    f.write(config_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_arguments():                                                                                                                                                                                                                                                \n",
    "#     parser = argparse.ArgumentParser(description='')                                                                                                                                                                                                                  \n",
    "#     parser.add_argument('pipeline')                                                                                                                                                                                                                                   \n",
    "#     parser.add_argument('output')                                                                                                                                                                                                                                     \n",
    "#     return parser.parse_args()                                                                                                                                                                                                                                        \n",
    "\n",
    "\n",
    "def main():                                                                                                                                                                                                                                                           \n",
    "    args = parse_arguments()                                                                                                                                                                                                                                          \n",
    "                                                                                                                                                                                                             \n",
    "\n",
    "    with tf.gfile.GFile(args.pipeline, \"r\") as f:                                                                                                                                                                                                                     \n",
    "        proto_str = f.read()                                                                                                                                                                                                                                          \n",
    "        text_format.Merge(proto_str, pipeline_config)                                                                                                                                                                                                                 \n",
    "\n",
    "    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = 300                                                                                                                                                                                          \n",
    "    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = 300                                                                                                                                                                                           \n",
    "\n",
    "    config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n",
    "    with tf.gfile.Open(args.output, \"wb\") as f:                                                                                                                                                                                                                       \n",
    "        f.write(config_text)                                                                                                                                                                                                                                          \n",
    "\n",
    "\n",
    "if __name__ == '__main__':                                                                                                                                                                                                                                            \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import config_util\n",
    "\n",
    "pipeline_config = config_util.get_configs_from_pipeline_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model {\n",
       "  center_net {\n",
       "    num_classes: 90\n",
       "    feature_extractor {\n",
       "      type: \"hourglass_104\"\n",
       "      channel_means: 104.01361846923828\n",
       "      channel_means: 114.03422546386719\n",
       "      channel_means: 119.91659545898438\n",
       "      channel_stds: 73.60276794433594\n",
       "      channel_stds: 69.89082336425781\n",
       "      channel_stds: 70.91507720947266\n",
       "      bgr_ordering: true\n",
       "    }\n",
       "    image_resizer {\n",
       "      keep_aspect_ratio_resizer {\n",
       "        min_dimension: 1024\n",
       "        max_dimension: 1024\n",
       "        pad_to_max_dimension: true\n",
       "      }\n",
       "    }\n",
       "    object_detection_task {\n",
       "      task_loss_weight: 1.0\n",
       "      offset_loss_weight: 1.0\n",
       "      scale_loss_weight: 0.10000000149011612\n",
       "      localization_loss {\n",
       "        l1_localization_loss {\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    object_center_params {\n",
       "      object_center_loss_weight: 1.0\n",
       "      classification_loss {\n",
       "        penalty_reduced_logistic_focal_loss {\n",
       "          alpha: 2.0\n",
       "          beta: 4.0\n",
       "        }\n",
       "      }\n",
       "      min_box_overlap_iou: 0.699999988079071\n",
       "      max_box_predictions: 100\n",
       "    }\n",
       "  }\n",
       "}\n",
       "train_config {\n",
       "  batch_size: 128\n",
       "  data_augmentation_options {\n",
       "    random_horizontal_flip {\n",
       "    }\n",
       "  }\n",
       "  data_augmentation_options {\n",
       "    random_adjust_hue {\n",
       "    }\n",
       "  }\n",
       "  data_augmentation_options {\n",
       "    random_adjust_contrast {\n",
       "    }\n",
       "  }\n",
       "  data_augmentation_options {\n",
       "    random_adjust_saturation {\n",
       "    }\n",
       "  }\n",
       "  data_augmentation_options {\n",
       "    random_adjust_brightness {\n",
       "    }\n",
       "  }\n",
       "  data_augmentation_options {\n",
       "    random_square_crop_by_scale {\n",
       "      scale_min: 0.6000000238418579\n",
       "      scale_max: 1.2999999523162842\n",
       "    }\n",
       "  }\n",
       "  optimizer {\n",
       "    adam_optimizer {\n",
       "      learning_rate {\n",
       "        cosine_decay_learning_rate {\n",
       "          learning_rate_base: 0.0010000000474974513\n",
       "          total_steps: 50000\n",
       "          warmup_learning_rate: 0.0002500000118743628\n",
       "          warmup_steps: 5000\n",
       "        }\n",
       "      }\n",
       "      epsilon: 1.0000000116860974e-07\n",
       "    }\n",
       "    use_moving_average: false\n",
       "  }\n",
       "  fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/ckpt-1\"\n",
       "  num_steps: 50000\n",
       "  max_number_of_boxes: 100\n",
       "  unpad_groundtruth_tensors: false\n",
       "  fine_tune_checkpoint_type: \"detection\"\n",
       "  fine_tune_checkpoint_version: V2\n",
       "}\n",
       "train_input_reader {\n",
       "  label_map_path: \"PATH_TO_BE_CONFIGURED/label_map.txt\"\n",
       "  tf_record_input_reader {\n",
       "    input_path: \"PATH_TO_BE_CONFIGURED/train2017-?????-of-00256.tfrecord\"\n",
       "  }\n",
       "}\n",
       "eval_config {\n",
       "  metrics_set: \"coco_detection_metrics\"\n",
       "  use_moving_averages: false\n",
       "  batch_size: 1\n",
       "}\n",
       "eval_input_reader {\n",
       "  label_map_path: \"PATH_TO_BE_CONFIGURED/label_map.txt\"\n",
       "  shuffle: false\n",
       "  num_epochs: 1\n",
       "  tf_record_input_reader {\n",
       "    input_path: \"PATH_TO_BE_CONFIGURED/val2017-?????-of-00032.tfrecord\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'gfile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-70cd41e437e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-70cd41e437e8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mpipeline_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainEvalPipelineConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mproto_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtext_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'gfile'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from google.protobuf import text_format\n",
    "from object_detection.protos import pipeline_pb2\n",
    "\n",
    "\n",
    "# def parse_arguments():                                                                                                                                                                                                                                                \n",
    "#     parser = argparse.ArgumentParser(description='')                                                                                                                                                                                                                  \n",
    "#     parser.add_argument('pipeline')                                                                                                                                                                                                                                   \n",
    "#     parser.add_argument('output')                                                                                                                                                                                                                                     \n",
    "#     return parser.parse_args()                                                                                                                                                                                                                                        \n",
    "\n",
    "\n",
    "def main():\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    file_path='/home/daisy/mydrive/web-dev/deep_learning_pipeline-train-inference-/training_controller/apis/configs/tf2/centernet_hourglass104_1024x1024_coco17_tpu-32.config'                                                                                                                                                                                                                                         \n",
    "    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()                                                                                                                                                                                                          \n",
    "\n",
    "    with tf.gfile.GFile(args.pipeline, \"r\") as f:                                                                                                                                                                                                                     \n",
    "        proto_str = f.read()                                                                                                                                                                                                                                          \n",
    "        text_format.Merge(proto_str, pipeline_config)                                                                                                                                                                                                                 \n",
    "\n",
    "    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = 300                                                                                                                                                                                          \n",
    "    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = 300                                                                                                                                                                                           \n",
    "\n",
    "    config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n",
    "    with tf.gfile.Open(args.output, \"wb\") as f:                                                                                                                                                                                                                       \n",
    "        f.write(config_text)                                                                                                                                                                                                                                          \n",
    "\n",
    "\n",
    "if __name__ == '__main__':                                                                                                                                                                                                                                            \n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-39-ae70657cc7c2>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-ae70657cc7c2>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    cd $TOOL_DIR\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "TOOL_DIR=tool/tf-models/research\n",
    "\n",
    "(\n",
    "   cd $TOOL_DIR\n",
    "   protoc object_detection/protos/*.proto --python_out=.\n",
    ")\n",
    "\n",
    "export PYTHONPATH=$PYTHONPATH:$TOOL_DIR:$TOOL_DIR/slim\n",
    "\n",
    "python3 edit_pipeline.py pipeline.config pipeline_new.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'train_input_reader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-2bca7930ef26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_input_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_record_input_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/tensorflow/models/data/train100.record'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'train_input_reader'"
     ]
    }
   ],
   "source": [
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[0] = '/tensorflow/models/data/train100.record'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'protoc_gen_validate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-65bc01faf14f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprotoc_gen_validate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValidationFailed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Foo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Bar\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'protoc_gen_validate'"
     ]
    }
   ],
   "source": [
    "\n",
    "from protoc_gen_validate.validator import validate, ValidationFailed\n",
    "\n",
    "p = Person(first_name=\"Foo\", last_name=\"Bar\", age=42)\n",
    "try:\n",
    "    validate(p)\n",
    "except ValidationFailed as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "from typing import Any, Iterable, List, Optional, Set, Text, Tuple, Union\n",
    "\n",
    "from tensorflow_data_validation import types\n",
    "from tensorflow_data_validation.utils import io_util\n",
    "\n",
    "from google.protobuf import descriptor\n",
    "from google.protobuf import text_format\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-data-validation\n",
      "  Downloading tensorflow_data_validation-1.0.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2\n",
      "  Downloading tensorflow-2.5.0-cp38-cp38-manylinux2010_x86_64.whl (454.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 454.4 MB 46 kB/s  eta 0:00:01    |██                              | 27.6 MB 6.3 MB/s eta 0:01:08     |███████████████████▌            | 277.2 MB 5.1 MB/s eta 0:00:35     |████████████████████▉           | 295.3 MB 6.1 MB/s eta 0:00:27\n",
      "\u001b[?25hRequirement already satisfied: absl-py<0.13,>=0.9 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from tensorflow-data-validation) (0.10.0)\n",
      "Requirement already satisfied: numpy<1.20,>=1.16 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from tensorflow-data-validation) (1.18.5)\n",
      "Collecting apache-beam[gcp]<3,>=2.29\n",
      "  Downloading apache_beam-2.29.0-cp38-cp38-manylinux2010_x86_64.whl (11.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.5 MB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six<2,>=1.12 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from tensorflow-data-validation) (1.15.0)\n",
      "Collecting tfx-bsl<1.1,>=1.0\n",
      "  Downloading tfx_bsl-1.0.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib<0.15,>=0.12\n",
      "  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow<3,>=1\n",
      "  Using cached pyarrow-2.0.0-cp38-cp38-manylinux2014_x86_64.whl (17.8 MB)\n",
      "Collecting tensorflow-metadata<1.1,>=1.0\n",
      "  Downloading tensorflow_metadata-1.0.0-py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 794 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas<2,>=1.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from tensorflow-data-validation) (1.1.5)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from tensorflow-data-validation) (3.14.0)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (2.5.8)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (3.11.2)\n",
      "Requirement already satisfied: oauth2client<5,>=2.0.1 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (4.1.3)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (0.3.1.1)\n",
      "Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (1.9.2.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.21.4 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (1.2.1)\n",
      "Requirement already satisfied: httplib2<0.18.0,>=0.8 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (0.17.4)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (1.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (2.25.0)\n",
      "Requirement already satisfied: grpcio<2,>=1.29.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (1.34.0)\n",
      "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (3.7.4.3)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (1.7)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (2.8.1)\n",
      "Requirement already satisfied: future<1.0.0,>=0.18.2 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (0.18.2)\n",
      "Requirement already satisfied: pytz>=2018.3 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (2020.4)\n",
      "Collecting google-cloud-datastore<2,>=1.7.1\n",
      "  Downloading google_cloud_datastore-1.15.3-py2.py3-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cachetools<5,>=3.1.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (4.2.0)\n",
      "Collecting grpcio-gcp<1,>=0.2.2\n",
      "  Downloading grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n",
      "Requirement already satisfied: google-auth<2,>=1.18.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (1.24.0)\n",
      "Collecting google-cloud-pubsub<2,>=0.39.0\n",
      "  Downloading google_cloud_pubsub-1.7.0-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[K     |████████████████████████████████| 144 kB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-apitools<0.5.32,>=0.5.31\n",
      "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
      "\u001b[K     |████████████████████████████████| 173 kB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-vision<2,>=0.38.0\n",
      "  Downloading google_cloud_vision-1.0.0-py2.py3-none-any.whl (435 kB)\n",
      "\u001b[K     |████████████████████████████████| 435 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-bigquery<2,>=1.6.0\n",
      "  Downloading google_cloud_bigquery-1.28.0-py2.py3-none-any.whl (187 kB)\n",
      "\u001b[K     |████████████████████████████████| 187 kB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-spanner<2,>=1.13.0\n",
      "  Downloading google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255 kB)\n",
      "\u001b[K     |████████████████████████████████| 255 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-dlp<2,>=0.12.0\n",
      "  Downloading google_cloud_dlp-1.0.0-py2.py3-none-any.whl (169 kB)\n",
      "\u001b[K     |████████████████████████████████| 169 kB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-bigtable<2,>=0.31.1\n",
      "  Downloading google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267 kB)\n",
      "\u001b[K     |████████████████████████████████| 267 kB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-language<2,>=1.3.0\n",
      "  Downloading google_cloud_language-1.3.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-videointelligence<2,>=1.8.0\n",
      "  Downloading google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183 kB)\n",
      "\u001b[K     |████████████████████████████████| 183 kB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (1.5.0)\n",
      "Collecting fasteners>=0.14\n",
      "  Downloading fasteners-0.16-py2.py3-none-any.whl (28 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (4.6)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (44.0.0)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (1.2.0)\n",
      "Requirement already satisfied: google-api-core<2.0dev,>=1.21.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (1.24.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from google-api-core<2.0dev,>=1.21.0->google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (1.52.0)\n",
      "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
      "  Downloading grpc-google-iam-v1-0.12.3.tar.gz (13 kB)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (1.14.4)\n",
      "Requirement already satisfied: pycparser in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<2,>=1.6.0->apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (2.20)\n",
      "Requirement already satisfied: docopt in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (0.6.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (0.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from pydot<2,>=1.2.0->apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (1.26.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.29->tensorflow-data-validation) (3.0.4)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation) (0.2.0)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing~=1.1.2 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation) (1.1.2)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation) (0.36.2)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation) (1.12.1)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation) (1.6.3)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation) (3.3.0)\n",
      "Collecting tensorboard~=2.5\n",
      "  Using cached tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "Collecting numpy<1.20,>=1.16\n",
      "  Using cached numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation) (0.4.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation) (3.3.3)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation) (3.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client<2,>=1.7.11 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from tfx-bsl<1.1,>=1.0->tensorflow-data-validation) (1.12.8)\n",
      "Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15\n",
      "  Downloading tensorflow_serving_api-2.5.1-py2.py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.1,>=1.0->tensorflow-data-validation) (0.0.4)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /home/daisy/mydrive/datascience/environments/main_env/lib/python3.8/site-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.1,>=1.0->tensorflow-data-validation) (3.0.1)\n",
      "Building wheels for collected packages: google-apitools, grpc-google-iam-v1\n",
      "  Building wheel for google-apitools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131042 sha256=bfc9e99a0a69f7c8f914bcbe6f72946dc6df152410cd47731dfc181e01629a5c\n",
      "  Stored in directory: /home/daisy/.cache/pip/wheels/d7/54/79/85de1824f2f4175fb4960c72afb10045d86700c3941dc73685\n",
      "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-py3-none-any.whl size=18482 sha256=fffb1c1a629b9776a0bea208115af836fd681fa8dc44d5279ef61b8aeb37b744\n",
      "  Stored in directory: /home/daisy/.cache/pip/wheels/8f/b9/13/fce3d62261f63c01b28281fe6a9d704a7af65d96ff2c88552e\n",
      "Successfully built google-apitools grpc-google-iam-v1\n",
      "Installing collected packages: tensorboard-data-server, numpy, grpcio-gcp, tensorflow-estimator, tensorboard, pyarrow, keras-nightly, h5py, grpc-google-iam-v1, gast, flatbuffers, fasteners, tensorflow, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-pubsub, google-cloud-language, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-cloud-bigquery, google-apitools, apache-beam, tensorflow-serving-api, tensorflow-metadata, tfx-bsl, joblib, tensorflow-data-validation\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "      Successfully uninstalled numpy-1.18.5\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.3.0\n",
      "    Uninstalling tensorflow-estimator-2.3.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.4.0\n",
      "    Uninstalling tensorboard-2.4.0:\n",
      "      Successfully uninstalled tensorboard-2.4.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 0.17.1\n",
      "    Uninstalling pyarrow-0.17.1:\n",
      "      Successfully uninstalled pyarrow-0.17.1\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.3.3\n",
      "    Uninstalling gast-0.3.3:\n",
      "      Successfully uninstalled gast-0.3.3\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.3.1\n",
      "    Uninstalling tensorflow-2.3.1:\n",
      "      Successfully uninstalled tensorflow-2.3.1\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 2.6.1\n",
      "    Uninstalling google-cloud-bigquery-2.6.1:\n",
      "      Successfully uninstalled google-cloud-bigquery-2.6.1\n",
      "  Attempting uninstall: apache-beam\n",
      "    Found existing installation: apache-beam 2.26.0\n",
      "    Uninstalling apache-beam-2.26.0:\n",
      "      Successfully uninstalled apache-beam-2.26.0\n",
      "  Attempting uninstall: tensorflow-metadata\n",
      "    Found existing installation: tensorflow-metadata 0.26.0\n",
      "    Uninstalling tensorflow-metadata-0.26.0:\n",
      "      Successfully uninstalled tensorflow-metadata-0.26.0\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.0.0\n",
      "    Uninstalling joblib-1.0.0:\n",
      "      Successfully uninstalled joblib-1.0.0\n",
      "Successfully installed apache-beam-2.29.0 fasteners-0.16 flatbuffers-1.12 gast-0.4.0 google-apitools-0.5.31 google-cloud-bigquery-1.28.0 google-cloud-bigtable-1.7.0 google-cloud-datastore-1.15.3 google-cloud-dlp-1.0.0 google-cloud-language-1.3.0 google-cloud-pubsub-1.7.0 google-cloud-spanner-1.19.1 google-cloud-videointelligence-1.16.1 google-cloud-vision-1.0.0 grpc-google-iam-v1-0.12.3 grpcio-gcp-0.2.2 h5py-3.1.0 joblib-0.14.1 keras-nightly-2.5.0.dev2021032900 numpy-1.19.5 pyarrow-2.0.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorflow-2.5.0 tensorflow-data-validation-1.0.0 tensorflow-estimator-2.5.0 tensorflow-metadata-1.0.0 tensorflow-serving-api-2.5.1 tfx-bsl-1.0.0\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/home/daisy/mydrive/datascience/environments/main_env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-data-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(schema: schema_pb2.Schema,\n",
    "                feature_path: Union[types.FeatureName, types.FeaturePath]\n",
    "               ) -> schema_pb2.Feature:\n",
    "  \"\"\"Get a feature from the schema.\n",
    "  Args:\n",
    "    schema: A Schema protocol buffer.\n",
    "    feature_path: The path of the feature to obtain from the schema. If a\n",
    "      FeatureName is passed, a one-step FeaturePath will be constructed and\n",
    "      used. For example, \"my_feature\" -> types.FeaturePath([\"my_feature\"])\n",
    "  Returns:\n",
    "    A Feature protocol buffer.\n",
    "  Raises:\n",
    "    TypeError: If the input schema is not of the expected type.\n",
    "    ValueError: If the input feature is not found in the schema.\n",
    "  \"\"\"\n",
    "  if not isinstance(schema, schema_pb2.Schema):\n",
    "    raise TypeError('schema is of type %s, should be a Schema proto.' %\n",
    "                    type(schema).__name__)\n",
    "\n",
    "  if not isinstance(feature_path, types.FeaturePath):\n",
    "    feature_path = types.FeaturePath([feature_path])\n",
    "\n",
    "  feature_container = schema.feature\n",
    "  parent = feature_path.parent()\n",
    "  if parent:\n",
    "    for step in parent.steps():\n",
    "      f = look_up_feature(step, feature_container)\n",
    "      if f is None:\n",
    "        raise ValueError('Feature %s not found in the schema.' % feature_path)\n",
    "      if f.type != schema_pb2.STRUCT:\n",
    "        raise ValueError(\n",
    "            'Step %s in feature %s does not refer to a valid STRUCT feature' %\n",
    "            (step, feature_path))\n",
    "      feature_container = f.struct_domain.feature\n",
    "\n",
    "  feature = look_up_feature(feature_path.steps()[-1], feature_container)\n",
    "  if feature is None:\n",
    "    raise ValueError('Feature %s not found in the schema.' % feature_path)\n",
    "  return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from google.protobuf import text_format\n",
    "from object_detection.protos import pipeline_pb2\n",
    "\n",
    "def get_feature(schema,feature_path):\n",
    "    if not isinstance(schema, schema_pb2.Schema):\n",
    "        raise TypeError('schema is of type %s, should be a Schema proto.' %\n",
    "                    type(schema).__name__)\n",
    "\n",
    "    if not isinstance(feature_path, types.FeaturePath):\n",
    "        feature_path = types.FeaturePath([feature_path])\n",
    "    \n",
    "    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()                                                                                                                                                                                                          \n",
    "    \n",
    "    with tf.io.gfile.GFile(feature_path, \"r\") as f:                                                                                                                                                                                                                     \n",
    "        proto_str = f.read()\n",
    "    print(schema)\n",
    "    print(proto_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "schema is of type GeneratedProtocolMessageType, should be a Schema proto.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-2e08a087668e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSchema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'configs/tf2/centernet_hourglass104_1024x1024_coco17_tpu-32.config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-52cf6fa1bb7c>\u001b[0m in \u001b[0;36mget_feature\u001b[0;34m(schema, feature_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         raise TypeError('schema is of type %s, should be a Schema proto.' %\n\u001b[0m\u001b[1;32m     10\u001b[0m                     type(schema).__name__)\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: schema is of type GeneratedProtocolMessageType, should be a Schema proto."
     ]
    }
   ],
   "source": [
    "get_feature(schema_pb2.Schema,'configs/tf2/centernet_hourglass104_1024x1024_coco17_tpu-32.config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
